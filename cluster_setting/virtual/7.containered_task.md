### What is container? What is its merit?
Read them https://and-engineer.com/articles/YZyl2REAACMAlPMj  , http://www.hpc.cmc.osaka-u.ac.jp/wp-content/uploads/2021/09/20211021.pdf  
As many users will use the cluster, software version conflict does matter.  
In addition, installing environment to every node is tiresome (Imagine you install matlab to 14 nodes manually).  
container solution solves this problem.


Here, we use singularity as an example of container system.  
We execute julia program (Julia is not installed in the nodes themselves) via singularity.


### Create container image
In r390x0 (in r395x1 also will do) ```singularity build /projects/controller/Julia.sif docker://julia``` to fetch julia-ready container and save it in ```/projects/controller```.  
As ```/projects``` is shared by all nodes, other nodes can use the image.
![Screenshot from 2022-07-30 19-42-06](https://user-images.githubusercontent.com/80142550/181907031-65f1c499-5bf0-45b5-ac13-1fe7a834a635.png)

### Execute container image
Let's read our sample programs.
``` julia.jl
println(ARGS)

num = ARGS[1]
open("/archive/controller/results/result_"*num, "w")  do fp
    write(fp, "sample text\n")
end
```
This julia program reads the first argument and save it as ```num``` variable.  
Then, write "sample text\n" to file ```"/archive/controller/results/result_${num}"```.
To execute this with singularity container, the command below will do.
```
singularity exec --bind /projects:/projects\
                 --bind /archive:/archive\
                 /projects/controller/JuliaJulia.sif\
                 julia /projects/lab_notes/cluster_setting/virtual/julia_sample.jl
```

The 1st argument of it, ```--bind /projects:/projects``` let singularity image can read/write host node's ```/archive```.  
The 2nd works similarly.  
The 3rd argument specifies what singluarity container image to be run, ```/projects/controller/JuliaJulia.sif```.  
The 4th argument commands that, julia in container image run ```/projects/lab_notes/cluster_setting/virtual/julia_sample.jl``` in ther container.  


To execute with slurm, the batch file is as follows.


```
#!/bin/bash
#SBATCH --job-name hostname
#SBATCH --output /archive/controller/logs/out/%x-%j.log
#SBATCH --error /archive/controller/logs/err/%x-%j.err
#SBATCH --cpus-per-task 1
#SBATCH --time 01:00
#SBATCH --partition main
sleep 30
singularity exec --bind /projects:/projects --bind /archive:/archive  --bind /archive:/archive /projects/controller/JuliaJulia.sif julia /projects/lab_notes/cluster_setting/virtual/julia_sample.jl $1
```

Command ```batch julia_batch.sh 1```, ```batch julia_batch.sh 2```, ```batch julia_batch.sh 3```. These tasks are queued to slurm.  
![Screenshot from 2022-07-30 20-32-09](https://user-images.githubusercontent.com/80142550/181908999-aa8682f4-6720-474c-8452-1ec62d8d9f68.png)

By ```sinfo```, check that the queue is scheduled to run by slurm.  

![Screenshot from 2022-07-30 20-33-15](https://user-images.githubusercontent.com/80142550/181909041-26c94a68-0690-4771-b05c-8470be59c42c.png)

Finally, check that result files are saved in ``/archive/controller/results```.


![Screenshot from 2022-07-30 20-34-57](https://user-images.githubusercontent.com/80142550/181909103-88fc4842-c91d-4000-bee6-b889ae40d032.png)





