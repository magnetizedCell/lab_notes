### Configure ```/etc/hosts```
Add lines below to ```/etc/hosts```. In real cluster system, add all nodes information in ```/etc/hosts```.  
**DO NOT** edit lines start with ```127.0.0.1``` and ```127.0.1.1```.

```
192.168.1.101 r390x0
192.168.1.101 r395x1
```

![Screenshot from 2022-07-30 17-53-24](https://user-images.githubusercontent.com/80142550/181903134-0c5e897b-06c0-4dcf-84b4-f742e23f7f97.png)

### Install slurm

```
sudo apt install slurm-wlm munge libmunge-dev
sudo echo slurm-wlm hold | sudo dpkg --set-selections
```

#### Create configuration file
slurm's configuration is written in ```/etc/slurm/slurm.conf```.
To create ```slurm.conf```, we can use configuration tool.
Download slurm package from ubuntu launchpad. Make sure the version you download is the same as you installed. At the time of writing, download slurm-wlm_21.08.5.orig.tar.gz from [this](https://launchpad.net/ubuntu/+source/slurm-wlm/21.08.5-1).  
Extract it and open ```doc/html/configurator.html.in```.
There, other than ```Control Machines```, ```Compute Machines``` and ```Process Tracking```, default setting will do. Read explanation carefully to create config for your environment. Select ```LinuxProc``` as ```ProctrackType```.  
If your have various types of compute nodes, yous must add the node manually to ```slurm.conf```.
In our virtual environment [```./slurm.conf```](https://github.com/magnetizedCell/lab_notes/blob/main/cluster_setting/virtual/slurm.conf) is enough. 

#### In r390x0 only
```munge.key``` must be shared among all nodes. Here we copy r390x0's one to other nodes.
```sudo cp /etc/munge/munge.key /projects/lab_notes/cluster_setting/virtual/``` to copy it to shared directory.
```sudo chmod 777 /projects/lab_notes/cluster_setting/virtual/munge.key``` to make it readable from another nodes.

#### In r395x1 only
```sudo cp /projects/lab_notes/cluster_setting/virtual/munge.key /etc/munge``` to copy the shared ```munge.key``` to r395x1  
munge has strict permission restriction for key file. Specify as it wishes.
```
sudo chown munge:munge /etc/munge/munge.key
sudo chmod 400 /etc/munge/munge.key
```
```sudo systemctl restart munge``` to load the key file.

#### Both nodes
reboot both nodes ```sudo reboot```.  
After reboot, ```sinfo``` to check the status of the cluster.
![Screenshot from 2022-07-30 18-17-43](https://user-images.githubusercontent.com/80142550/181903978-cf61c8cc-5076-4e87-837a-87a0347192c1.png)

If something is bad (e.g. STATE of some node is unk or down), then command ```sudo systemctl restart slurmd``` in the problematic node and retry ```sinfo```.  
If problem do not be solved, consult ```sudo systemctl status slurmd``` in worker node and ```sudo systemctl status slurmctld```  
Read error messages.  
![Screenshot from 2022-07-30 18-20-54](https://user-images.githubusercontent.com/80142550/181904125-05223e2b-527f-4952-9c16-27047e20f4f0.png)
(example of healthy slurmctld status)
