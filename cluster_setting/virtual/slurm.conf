# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=r390x0
MpiDefault=none
ProctrackType=proctrack/linuxproc
ReturnToService=1
SlurmctldPidFile=/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/slurmd
SlurmUser=slurm
StateSaveLocation=/var/spool/slurm
SwitchType=switch/none
TaskPlugin=task/affinity
InactiveLimit=0
KillWait=30
MinJobAge=300
Waittime=0

SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core
AccountingStorageType=accounting_storage/none
AccountingStoreFlags=job_comment
ClusterName=cluster
JobCompType=jobcomp/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=info
SlurmdDebug=info
SuspendProgram=/usr/bin/suspend_worker
ResumeProgram=/usr/bin/resume_worker
SuspendTimeout=120
ResumeTimeout=120
ResumeRate=2
SuspendExcNodes=r390x0
SuspendRate=2
SuspendTime=10


# COMPUTE NODES
NodeName=r390x0 CPUs=1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
NodeName=r395x1 CPUs=1 Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
#NodeName=r390x3 CPUs=24 Sockets=1 CoresPerSocket=12 ThreadsPerCore=2 State=UNKNOWN
#NodeName=p100 CPUs=36 Sockets=1 CoresPerSocket=18 ThreadsPerCore=2 State=UNKNOWN
#NodeName=epyc1 CPUs=128 Sockets=2 CoresPerSocket=64 ThreadsPerCore=1 State=UNKNOWN
PartitionName=debug Nodes=r390x0 Default=YES MaxTime=INFINITE State=UP
PartitionName=main Nodes=ALL MaxTime=INFINITE State=UP
